---
layout: post
title:  pyspark模板
date: 2021-02-10
tags: 博客
---

pyspark主要在python脚本里书写逻辑，然后直接调用即可。

## 1. python计算脚本：calc.py

	# -*- coding:utf-8 -*-
	import time
	import json
	import math
	import logging
	import subprocess
	from collections import Counter,defaultdict
	from pyspark import StorageLevel
	from pyspark import SparkContext
	import base64
	import hashlib
	import numpy as np
	import datetime
	from utils import save_data,load_data,scatter_line,cal_all_metrics
	
	print "Start time: ", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
	
	def datask_run(datask):
	    # log
	    logger = logging.getLogger("custom")
	    logger.setLevel(logging.INFO)
	    logformat = logging.Formatter('%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s','%a, %d %b %Y %H:%M:%S')
	    ch = logging.StreamHandler()
	    ch.setFormatter(logformat)
	    logger.addHandler(ch)
	    logger.info("pyspark demo test")
	    
	    ### input and output ###
	    method = "rand2"
	    sk_input = "hdfs://xxxx/input/20200627/00/*"
	    sk_output = "hdfs://xxxx/output/20200627/00"
	    print ">>>>>>>>>sk_input_path:", sk_input
	    print ">>>>>>>>>sk_output_path:", sk_output
	    ### clear ###
	    subprocess.call("hadoop fs -rm -r "+sk_output, shell=True)
	
	    context = datask
	    
	    ### process ###
	    rdd_sum = load_data(context, sk_input) \
	            .map(lambda x: scatter_line(x)) \
	            .groupByKey().mapValues(list) \
	            .map(lambda x: cal_all_metrics(x))
	    #print(rdd_sum.collect())
	    save_data(rdd_sum, sk_output)
	
	    print "End time: ", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
	
	
	if __name__ == "__main__":
	    time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
	    app_name = "[xxx]"
	    sc = SparkContext(appName=app_name)
	    datask_run(datask=sc)
	    
	    
## 2. 调用

脚本说明：calc.py在这里传给spark-submit，utils.py被calc.py调用。

	#!/bin/bash
	source /data9/ericoliang/kbemr_env.sh
	
	spark-submit \
	    --class org.apache.spark.examples.SparkPi \
	    --master yarn \
	    --deploy-mode client \
	    --driver-memory 6G \
	    --executor-memory 6G \
	    --executor-cores 2 \ 
	    --num-executors 200 \
	    --queue low \
	    --conf spark.default.parallelism=200 \
	    --conf spark.executor.memoryOverhead=6G \
	    --conf spark.driver.maxResultSize=0 \
	    --py-files utils.py \
	    calc.py